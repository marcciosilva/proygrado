{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import parser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import generar_jobs\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the problem parameters are established (should make this dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO receive as parameters\n",
    "task_amount = 128\n",
    "machine_amount = 4\n",
    "task_heterogeneity = 0\n",
    "machine_heterogeneity = 0\n",
    "consistency_type = 0\n",
    "accuracy_scores = []\n",
    "classifiers = []\n",
    "# Classifier configuration.\n",
    "USING_ENTIRE_ETC = True\n",
    "CLASSIFIER_STRING_ANN = 'ann'\n",
    "CLASSIFIER_STRING_SVM = 'svm'\n",
    "classifier_types = [CLASSIFIER_STRING_ANN, CLASSIFIER_STRING_SVM]\n",
    "current_classifier_index = 0 # Only modify this.\n",
    "current_classifier_str = classifier_types[current_classifier_index]\n",
    "# Base path for classifier persistence.\n",
    "model_base_path = './models/' + current_classifier_str + '/' + str(task_amount) + 'x' + str(machine_amount) \\\n",
    "    + '-' + str(task_heterogeneity) + str(machine_heterogeneity) \\\n",
    "    + str(consistency_type) + '/'\n",
    "baseDir = './data-processed/' + str(task_amount) + 'x' \\\n",
    "    + str(machine_amount) + '-' + str(task_heterogeneity) \\\n",
    "    + str(machine_heterogeneity) + str(consistency_type) + '/'\n",
    "model_file_prefix = 'clf-' + current_classifier_str\n",
    "model_file_extension = '.pkl'\n",
    "\n",
    "if current_classifier_str == CLASSIFIER_STRING_ANN:\n",
    "    if USING_ENTIRE_ETC:\n",
    "        dimension = task_amount * machine_amount\n",
    "        # Reference: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "        ns = 600  # Amount of training examples.\n",
    "        ni = dimension\n",
    "        no = 1  # Amount of output neurons.\n",
    "        alpha = 2\n",
    "        hidden_layer_amount = 2 #int(math.ceil(ns / (alpha * (ni + no)))) # Con 2 hardcodeado parece aprender mejor\n",
    "        # Each hidden layer has an intermediate amount of neurons (between the neuron amount\n",
    "        # present in the output layer and the input layer).\n",
    "        # A tuple is generated to set up the MLPClassifier.\n",
    "        hidden_layer_neuron_amount = tuple([int(math.ceil((task_amount - no) / 2))]\n",
    "                                           * hidden_layer_amount)  \n",
    "    else:\n",
    "        dimension = machine_amount\n",
    "        # Reference: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "        ns = 600  # Amount of training examples.\n",
    "        ni = dimension\n",
    "        no = 1  # Amount of output neurons.\n",
    "        alpha = 2\n",
    "        hidden_layer_amount = 2 #int(math.ceil(ns / (alpha * (ni + no)))) # Con 2 hardcodeado parece aprender mejor\n",
    "        # Each hidden layer has an intermediate amount of neurons (between the neuron amount\n",
    "        # present in the output layer and the input layer).\n",
    "        # A tuple is generated to set up the MLPClassifier.\n",
    "        hidden_layer_neuron_amount = tuple([int(math.ceil((ni - no) / 2))]\n",
    "                                           * hidden_layer_amount) \n",
    "elif current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "    # No mandatory config for SVC method.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, classifiers are loaded (or generated if they don't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier for output 0 didn't exist.\n",
      "The classifier for output 1 didn't exist.\n",
      "The classifier for output 2 didn't exist.\n",
      "The classifier for output 3 didn't exist.\n",
      "The classifier for output 4 didn't exist.\n",
      "The classifier for output 5 didn't exist.\n",
      "The classifier for output 6 didn't exist.\n",
      "The classifier for output 7 didn't exist.\n",
      "The classifier for output 8 didn't exist.\n",
      "The classifier for output 9 didn't exist.\n",
      "The classifier for output 10 didn't exist.\n",
      "The classifier for output 11 didn't exist.\n",
      "The classifier for output 12 didn't exist.\n",
      "The classifier for output 13 didn't exist.\n",
      "The classifier for output 14 didn't exist.\n",
      "The classifier for output 15 didn't exist.\n",
      "The classifier for output 16 didn't exist.\n",
      "The classifier for output 17 didn't exist.\n",
      "The classifier for output 18 didn't exist.\n",
      "The classifier for output 19 didn't exist.\n",
      "The classifier for output 20 didn't exist.\n",
      "The classifier for output 21 didn't exist.\n",
      "The classifier for output 22 didn't exist.\n",
      "The classifier for output 23 didn't exist.\n",
      "The classifier for output 24 didn't exist.\n",
      "The classifier for output 25 didn't exist.\n",
      "The classifier for output 26 didn't exist.\n",
      "The classifier for output 27 didn't exist.\n",
      "The classifier for output 28 didn't exist.\n",
      "The classifier for output 29 didn't exist.\n",
      "The classifier for output 30 didn't exist.\n",
      "The classifier for output 31 didn't exist.\n",
      "The classifier for output 32 didn't exist.\n",
      "The classifier for output 33 didn't exist.\n",
      "The classifier for output 34 didn't exist.\n",
      "The classifier for output 35 didn't exist.\n",
      "The classifier for output 36 didn't exist.\n",
      "The classifier for output 37 didn't exist.\n",
      "The classifier for output 38 didn't exist.\n",
      "The classifier for output 39 didn't exist.\n",
      "The classifier for output 40 didn't exist.\n",
      "The classifier for output 41 didn't exist.\n",
      "The classifier for output 42 didn't exist.\n",
      "The classifier for output 43 didn't exist.\n",
      "The classifier for output 44 didn't exist.\n",
      "The classifier for output 45 didn't exist.\n",
      "The classifier for output 46 didn't exist.\n",
      "The classifier for output 47 didn't exist.\n",
      "The classifier for output 48 didn't exist.\n",
      "The classifier for output 49 didn't exist.\n",
      "The classifier for output 50 didn't exist.\n",
      "The classifier for output 51 didn't exist.\n",
      "The classifier for output 52 didn't exist.\n",
      "The classifier for output 53 didn't exist.\n",
      "The classifier for output 54 didn't exist.\n",
      "The classifier for output 55 didn't exist.\n",
      "The classifier for output 56 didn't exist.\n",
      "The classifier for output 57 didn't exist.\n",
      "The classifier for output 58 didn't exist.\n",
      "The classifier for output 59 didn't exist.\n",
      "The classifier for output 60 didn't exist.\n",
      "The classifier for output 61 didn't exist.\n",
      "The classifier for output 62 didn't exist.\n",
      "The classifier for output 63 didn't exist.\n",
      "The classifier for output 64 didn't exist.\n",
      "The classifier for output 65 didn't exist.\n",
      "The classifier for output 66 didn't exist.\n",
      "The classifier for output 67 didn't exist.\n",
      "The classifier for output 68 didn't exist.\n",
      "The classifier for output 69 didn't exist.\n",
      "The classifier for output 70 didn't exist.\n",
      "The classifier for output 71 didn't exist.\n",
      "The classifier for output 72 didn't exist.\n",
      "The classifier for output 73 didn't exist.\n",
      "The classifier for output 74 didn't exist.\n",
      "The classifier for output 75 didn't exist.\n",
      "The classifier for output 76 didn't exist.\n",
      "The classifier for output 77 didn't exist.\n",
      "The classifier for output 78 didn't exist.\n",
      "The classifier for output 79 didn't exist.\n",
      "The classifier for output 80 didn't exist.\n",
      "The classifier for output 81 didn't exist.\n",
      "The classifier for output 82 didn't exist.\n",
      "The classifier for output 83 didn't exist.\n",
      "The classifier for output 84 didn't exist.\n",
      "The classifier for output 85 didn't exist.\n",
      "The classifier for output 86 didn't exist.\n",
      "The classifier for output 87 didn't exist.\n",
      "The classifier for output 88 didn't exist.\n",
      "The classifier for output 89 didn't exist.\n",
      "The classifier for output 90 didn't exist.\n",
      "The classifier for output 91 didn't exist.\n",
      "The classifier for output 92 didn't exist.\n",
      "The classifier for output 93 didn't exist.\n",
      "The classifier for output 94 didn't exist.\n",
      "The classifier for output 95 didn't exist.\n",
      "The classifier for output 96 didn't exist.\n",
      "The classifier for output 97 didn't exist.\n",
      "The classifier for output 98 didn't exist.\n",
      "The classifier for output 99 didn't exist.\n",
      "The classifier for output 100 didn't exist.\n",
      "The classifier for output 101 didn't exist.\n",
      "The classifier for output 102 didn't exist.\n",
      "The classifier for output 103 didn't exist.\n",
      "The classifier for output 104 didn't exist.\n",
      "The classifier for output 105 didn't exist.\n",
      "The classifier for output 106 didn't exist.\n",
      "The classifier for output 107 didn't exist.\n",
      "The classifier for output 108 didn't exist.\n",
      "The classifier for output 109 didn't exist.\n",
      "The classifier for output 110 didn't exist.\n",
      "The classifier for output 111 didn't exist.\n",
      "The classifier for output 112 didn't exist.\n",
      "The classifier for output 113 didn't exist.\n",
      "The classifier for output 114 didn't exist.\n",
      "The classifier for output 115 didn't exist.\n",
      "The classifier for output 116 didn't exist.\n",
      "The classifier for output 117 didn't exist.\n",
      "The classifier for output 118 didn't exist.\n",
      "The classifier for output 119 didn't exist.\n",
      "The classifier for output 120 didn't exist.\n",
      "The classifier for output 121 didn't exist.\n",
      "The classifier for output 122 didn't exist.\n",
      "The classifier for output 123 didn't exist.\n",
      "The classifier for output 124 didn't exist.\n",
      "The classifier for output 125 didn't exist.\n",
      "The classifier for output 126 didn't exist.\n",
      "The classifier for output 127 didn't exist.\n"
     ]
    }
   ],
   "source": [
    "# TODO maybe specify classifier configuration along with this (so as to not specify something that might already exist)\n",
    "for i in range(0, task_amount):\n",
    "    try:\n",
    "        classifier = joblib.load(model_base_path + model_file_prefix + str(i) \\\n",
    "                                 + model_file_extension)\n",
    "    except Exception:\n",
    "        print('The classifier for output ' + str(i) + ' didn\\'t exist.')\n",
    "        if current_classifier_str == CLASSIFIER_STRING_ANN:\n",
    "            classifier = MLPClassifier(solver='lbfgs', alpha=1e-2, \n",
    "                hidden_layer_sizes=hidden_layer_neuron_amount, random_state=1)\n",
    "        elif current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "            classifier = svm.SVC()\n",
    "    finally:\n",
    "        # Append classifier to classifier list (in memory).\n",
    "        classifiers.append(classifier)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, each classifier is trained, and the following is done for each one (after training it):\n",
    "* Its accuracy is determined using the training set\n",
    "* Each training set instance is iterated over, and the time each machine uses in execution is stored (for calculating the makespan afterwards, in another section) in an array\n",
    "    * Each entry of the array will be an array of machine_amount elements, in which each element corresponds to the time each machine uses up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier 0...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.34\n",
      "Training classifier 1...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 2...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 3...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 4...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 5...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 6...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 7...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 8...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 9...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 10...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 11...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 12...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 13...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 14...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 15...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.48\n",
      "Training classifier 16...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 17...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 18...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 19...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 20...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 21...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 22...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 23...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.33\n",
      "Training classifier 24...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.34\n",
      "Training classifier 25...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 26...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 27...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 28...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.48\n",
      "Training classifier 29...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 30...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.34\n",
      "Training classifier 31...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 32...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 33...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.33\n",
      "Training classifier 34...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 35...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 36...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 37...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 38...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 39...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 40...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.52\n",
      "Training classifier 41...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 42...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 43...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.32\n",
      "Training classifier 44...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 45...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 46...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 47...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 48...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 49...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.33\n",
      "Training classifier 50...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.31\n",
      "Training classifier 51...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 52...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 53...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 54...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 55...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 56...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 57...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.3\n",
      "Training classifier 58...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 59...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.32\n",
      "Training classifier 60...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 61...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 62...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 63...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 64...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.3\n",
      "Training classifier 65...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 66...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 67...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 68...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 69...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 70...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 71...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 72...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.28\n",
      "Training classifier 73...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.32\n",
      "Training classifier 74...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 75...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 76...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.48\n",
      "Training classifier 77...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 78...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 79...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 80...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 81...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 82...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 83...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 84...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.31\n",
      "Training classifier 85...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 86...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.46\n",
      "Training classifier 87...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.32\n",
      "Training classifier 88...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "Training classifier 89...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.39\n",
      "Training classifier 90...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 91...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 92...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.37\n",
      "Training classifier 93...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 94...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 95...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 96...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 97...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 98...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 99...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.53\n",
      "Training classifier 100...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.46\n",
      "Training classifier 101...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 102...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 103...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 104...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 105...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 106...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 107...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 108...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.38\n",
      "Training classifier 109...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 110...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.42\n",
      "Training classifier 111...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.49\n",
      "Training classifier 112...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 113...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 114...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.33\n",
      "Training classifier 115...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 116...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 117...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.44\n",
      "Training classifier 118...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.33\n",
      "Training classifier 119...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 120...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.43\n",
      "Training classifier 121...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.52\n",
      "Training classifier 122...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.36\n",
      "Training classifier 123...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 124...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.35\n",
      "Training classifier 125...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.41\n",
      "Training classifier 126...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.4\n",
      "Training classifier 127...\n",
      "    Doing makespan stuff...\n",
      "    Done with makespan stuff...\n",
      "    Classifier accuracy: 0.45\n",
      "The execution took 97.1688358784 seconds\n"
     ]
    }
   ],
   "source": [
    "# No threading version.\n",
    "start = time.time()\n",
    "# Each index corresponds to an instance.\n",
    "makespan_instance_machines_heuristic = []\n",
    "makespan_instance_machines_prediction = []\n",
    "SCALE_DATA = True\n",
    "USE_PARAMETER_SELECTION = False\n",
    "# Within each index, there'll be an array of machine_amount elements, in which each element\n",
    "# is the time during which each machine is running\n",
    "# Something along the lines of [[10,20,9,40], [99,88,22,11], ..., [10,9,21,35]]\n",
    "for i in range(0, task_amount): # For each task/classifier\n",
    "    print(\"Training classifier \" + str(i) + \"...\")\n",
    "    # Data is loaded.\n",
    "    TRAINING_FILE = baseDir + 'training/' + str(i) + '.csv' # Training file for current classifier\n",
    "    TEST_FILE = baseDir + 'test/' + str(i) + '.csv' # Test file for current classifier\n",
    "    training_set = pd.read_csv(TRAINING_FILE, header=None, delimiter=',')\n",
    "    test_set = pd.read_csv(TEST_FILE, header=None, delimiter=',')\n",
    "    \n",
    "    # Create dataframe for data and separate target.\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    df_training_input = df_training.iloc[:, :-1] # Leave rows alone, slice everything except last column.\n",
    "    # If not using entire ETC,use only the column relevant to the task/classifier.\n",
    "    if not USING_ENTIRE_ETC:\n",
    "        df_training_input = df_training_input.iloc[:, i * machine_amount : i * machine_amount + machine_amount]\n",
    "    df_training_output = df_training.iloc[:, -1]\n",
    "    \n",
    "    # Validation/testing data is loaded.\n",
    "    df_test = pd.DataFrame(test_set)\n",
    "    df_test_input = df_test.iloc[:, :-1]\n",
    "    # If not using entire ETC,use only the column relevant to the task/classifier.\n",
    "    if not USING_ENTIRE_ETC:\n",
    "        df_test_input = df_test_input.iloc[:, i * machine_amount : i * machine_amount + machine_amount]    \n",
    "    df_test_output = df_test.iloc[:, -1]\n",
    "    if SCALE_DATA:\n",
    "        # Scale data because http://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use\n",
    "        scaler = StandardScaler()  \n",
    "        # Fit only on training data.\n",
    "        scaler.fit(df_training_input)\n",
    "        # Reconvert input training data to dataframe after scaling (which converts it to an array of arrays).\n",
    "        df_training_input = pd.DataFrame(scaler.transform(df_training_input))\n",
    "        # Re-init scaler just in case.\n",
    "        scaler = StandardScaler()  \n",
    "        scaler.fit(df_test_input)\n",
    "        # Scale test data.\n",
    "        df_test_input = pd.DataFrame(scaler.transform(df_test_input))        \n",
    "    if current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "        if USE_PARAMETER_SELECTION:\n",
    "            # Grid of parameters, including all posible parameters for each configuration of\n",
    "            # an SVM classifier.\n",
    "            param_grid = [\n",
    "              {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    "             ]\n",
    "            # Run grid search with all the possible classifier configurations.\n",
    "            classifiers[i] = GridSearchCV(classifiers[i], param_grid=param_grid)\n",
    "            # This generates multiple estimators.\n",
    "            # Now the prediction will use the best estimator of all.\n",
    "            # Should use grid_search as new classifier, persist it, and use it for prediction\n",
    "            # as a normal classifier (according to documentation it uses the best estimator)\n",
    "            # However, it fits every possible estimator with the data, so that's something of note.            \n",
    "    # Classifier is trained using the data.\n",
    "    classifiers[i].fit(df_training_input, df_training_output)\n",
    "    # Classifier directory is generated if it doesn't exist.\n",
    "    generar_jobs.generate_dir(model_base_path)\n",
    "    # Classifier is persisted.\n",
    "    joblib.dump(classifiers[i], model_base_path + model_file_prefix + str(i) \\\n",
    "                + model_file_extension)\n",
    "    # Classifier accuracy is determined using test data.\n",
    "    results = []\n",
    "    # Go through every test instance manually to calculate makespan for each\n",
    "    # problem-classifier/task pair\n",
    "    current_task_index = i * machine_amount # Column index within etc matrix\n",
    "    print(\"    Doing makespan stuff...\")\n",
    "    test_instance_amount = len(df_test)\n",
    "    for j in range(0, test_instance_amount): # For every validation instance\n",
    "        if USING_ENTIRE_ETC:\n",
    "            # df_test.iloc[j] is an ETC matrix + the corresponding classification for one task\n",
    "            etc_matrix_scaled = df_test_input.iloc[j] # Scaled data for classification (since classifiers were\n",
    "            # trained using scaled data)\n",
    "            # Non-scaled data is used to calculate real makespan, using the original units of the problem.\n",
    "            etc_matrix = df_test.iloc[j][:-1] # Get j problem instance, ignoring last column (the output/classification).\n",
    "            classification_heuristic = float(df_test_output[j])\n",
    "            # Every test example is classified, and its classification is appended\n",
    "            # to a results array.\n",
    "            # Make prediction for current problem instance or etc matrix (using scaled data).\n",
    "            prediction_pandas = float(classifiers[i].predict(etc_matrix_scaled.values.reshape(1, -1)))\n",
    "            results.append(prediction_pandas)\n",
    "            prediction = float(prediction_pandas) # To work in floats.\n",
    "\n",
    "            # Get subrow from original input data, to get the task/machine times right.\n",
    "            sub_row_for_current_task = etc_matrix[current_task_index:current_task_index + machine_amount]\n",
    "            # Makespan value for prediction\n",
    "            current_makespan_prediction = sub_row_for_current_task[current_task_index + prediction]\n",
    "            # Makespan value for heuristic\n",
    "            current_makespan_heuristic = sub_row_for_current_task[current_task_index + classification_heuristic]\n",
    "            if len(makespan_instance_machines_prediction) <= j: # If there's no entry for this problem instance.\n",
    "                # Init entry for problem instance, with each machine's makespan starting at 0.0.\n",
    "                makespan_instance_machines_prediction.append([0.0] * machine_amount)\n",
    "                makespan_instance_machines_heuristic.append([0.0] * machine_amount)\n",
    "            makespan_instance_machines_prediction[j][int(prediction)] += current_makespan_prediction\n",
    "            makespan_instance_machines_heuristic[j][int(classification_heuristic)] += current_makespan_heuristic\n",
    "        else:\n",
    "            # df_test.iloc[j] is an ETC matrix + the corresponding classification for one task\n",
    "            sub_row_for_current_task_scaled = df_test_input.iloc[j] # Scaled data for classification (since classifiers were\n",
    "            # trained using scaled data)\n",
    "            # Non-scaled data is used to calculate real makespan, using the original units of the problem.\n",
    "#             etc_matrix = df_test.iloc[j][:-1] # Get j problem instance, ignoring last column (the output/classification).\n",
    "#             print(\"A: \")\n",
    "#             print(etc_matrix_scaled)\n",
    "#             print(\"B: \")\n",
    "            sub_row_for_current_task = df_test.iloc[:, :-1].iloc[j, i * machine_amount : i * machine_amount + machine_amount]                \n",
    "#             print(etc_matrix)\n",
    "            classification_heuristic = float(df_test_output[j])\n",
    "            # Every test example is classified, and its classification is appended\n",
    "            # to a results array.\n",
    "            # Make prediction for current problem instance or etc matrix (using scaled data).\n",
    "            prediction_pandas = float(classifiers[i].predict(sub_row_for_current_task_scaled.values.reshape(1, -1)))\n",
    "            results.append(prediction_pandas)\n",
    "            prediction = float(prediction_pandas) # To work in floats.\n",
    "\n",
    "            # Makespan value for prediction\n",
    "            current_makespan_prediction = sub_row_for_current_task[current_task_index + prediction]\n",
    "            # Makespan value for heuristic\n",
    "            current_makespan_heuristic = sub_row_for_current_task[current_task_index + classification_heuristic]\n",
    "            if len(makespan_instance_machines_prediction) <= j: # If there's no entry for this problem instance.\n",
    "                # Init entry for problem instance, with each machine's makespan starting at 0.0.\n",
    "                makespan_instance_machines_prediction.append([0.0] * machine_amount)\n",
    "                makespan_instance_machines_heuristic.append([0.0] * machine_amount)\n",
    "            makespan_instance_machines_prediction[j][int(prediction)] += current_makespan_prediction\n",
    "            makespan_instance_machines_heuristic[j][int(classification_heuristic)] += current_makespan_heuristic            \n",
    "    print(\"    Done with makespan stuff...\")\n",
    "    # Actual classification results are compared to expected values.\n",
    "    accuracy = accuracy_score(df_test_output, results)\n",
    "    print(\"    Classifier accuracy: \" + str(accuracy))\n",
    "    # Calculated accuracy is added to accuracies list.\n",
    "    accuracy_scores.append(accuracy)\n",
    "end = time.time()\n",
    "print('The execution took ' + str(end - start) + ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section takes the makespan data (which determines how much time each machine takes for each problem instance) and determines an average makespan for all of the problem instances (how much time the slowest machine takes in completing the tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference between techniques: 2830.8644\n",
      "The heuristic works better on average\n"
     ]
    }
   ],
   "source": [
    "# Array that holds makespan values for the prediction.\n",
    "makespan_prediction = []\n",
    "for i in range(0, len(makespan_instance_machines_prediction)):\n",
    "    makespan_prediction.append(np.max(makespan_instance_machines_prediction[i]))\n",
    "# Array that holds makespan values for the heuristic\n",
    "makespan_heuristic = []\n",
    "for i in range(0, len(makespan_instance_machines_heuristic)):\n",
    "    makespan_heuristic.append(np.max(makespan_instance_machines_heuristic[i]))\n",
    "# Array that holds the difference between heuristic and prediction makespan.\n",
    "makespan_diff = []\n",
    "for i in range(0, len(makespan_prediction)):\n",
    "    makespan_diff.append(makespan_prediction[i] - makespan_heuristic[i])\n",
    "# Calculate average difference between methods.\n",
    "avg_difference_between_methods = np.mean(makespan_diff)\n",
    "print('Average difference between techniques: ' + str(avg_difference_between_methods))\n",
    "if avg_difference_between_methods > 0:\n",
    "    print('The heuristic works better on average')\n",
    "elif avg_difference_between_methods < 0:\n",
    "    print('Savant works better on average')\n",
    "else:\n",
    "    print('Both techniques work equivalently on average')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section has multithreading code, needs to be reviewed and updated to match the non-multithreading version of the code (besides it isn't certain that this actually works on a cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO update this to include makespan calculation\n",
    "\n",
    "# # Multithreading version\n",
    "# import os\n",
    "\n",
    "# def train_and_persist_classifier(classifier_index):\n",
    "#     # Data is loaded.\n",
    "#     TRAINING_FILE = baseDir + 'training/' + str(classifier_index) + '.csv'\n",
    "#     TEST_FILE = baseDir + 'test/' + str(classifier_index) + '.csv'\n",
    "#     training_set = pd.read_csv(TEST_FILE, header=None, delimiter=',')\n",
    "#     test_set = pd.read_csv(TRAINING_FILE, header=None, delimiter=',')\n",
    "#     # Create dataframe for data and separate target.\n",
    "#     df_training = pd.DataFrame(training_set)\n",
    "#     y_training = df_training.iloc[:, -1]\n",
    "#     # Validation/testing data is loaded.\n",
    "#     df_test = pd.DataFrame(test_set)\n",
    "#     y_test = df_test.iloc[:, -1]\n",
    "#     # Classifier is trained using the data.\n",
    "#     classifiers[classifier_index].fit(df_training.iloc[:, :-1], y_training)\n",
    "#     # Classifier directory is generated if it doesn't exist.\n",
    "#     generar_jobs.generate_dir(model_base_path)\n",
    "#     # Classifier is persisted.\n",
    "#     joblib.dump(classifiers[classifier_index], model_base_path + model_file_prefix + str(classifier_index) \\\n",
    "#                 + model_file_extension)\n",
    "#     # Classifier accuracy is determined using test data.\n",
    "#     results = []\n",
    "#     for i in range(0, len(df_test)):\n",
    "#         # Every test example is classified, and its classification is appended\n",
    "#         # to a results array.\n",
    "#         results.append(classifiers[classifier_index].predict(\n",
    "#             df_test.iloc[i][:-1].values.reshape(1, -1)))\n",
    "#     # Actual results are compared to expected values.\n",
    "#     accuracy = accuracy_score(y_test, results)\n",
    "#     os.write(1,'Classifier ' + str(classifier_index) + ':\\n') # Print directly to console\n",
    "#     os.write(1, 'Accuracy: ' + str(accuracy) + ', ') # Print directly to console\n",
    "#     # Calculated accuracy is added to accuracies list.\n",
    "#     accuracy_scores.append(accuracy)\n",
    "# #     os.write(1, 'Training of classifier ' + str(classifier_index) + ' finished.\\n') \n",
    "#     return\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     ##### VERSION 1 #####\n",
    "# #     jobs = []\n",
    "# #     for i in range(0, task_amount):\n",
    "# #         print('Starting training of classifier ' + str(i))\n",
    "# #         p = multiprocessing.Process(target=train_and_persist_classifier(i))\n",
    "# #         jobs.append(p)\n",
    "# #         p.start()\n",
    "#     ##### END VERSION 1 #####\n",
    "#     ##### VERSION 2 #####\n",
    "#     start = time.time()\n",
    "#     num_cores = multiprocessing.cpu_count() * 4\n",
    "#     # For every task, train a classifier.\n",
    "#     Parallel(n_jobs=num_cores)(delayed(train_and_persist_classifier)(i) for i in range(0,task_amount))\n",
    "#     end = time.time()\n",
    "#     print('The execution took ' + str(end - start) + ' seconds')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section determines the average accuracy for the created classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy is 0.395390625\n"
     ]
    }
   ],
   "source": [
    "# Average accuracy (for all classifiers) is calculated (nothing to do with threading).\n",
    "promedio = 0.\n",
    "score_amount = len(accuracy_scores)\n",
    "for i in range(0, score_amount):\n",
    "    promedio += accuracy_scores[i]\n",
    "promedio /= score_amount\n",
    "print ('The average accuracy is {}'.format(promedio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logToConsole(msg):\n",
    "    '''\n",
    "    Logs messages to console from within a Jupyter Notebook.\n",
    "    '''\n",
    "    os.write(1, msg + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier 0...\n",
      "        0       1       2       3       4       5       6       7       8    \\\n",
      "0    180.80  242.71  495.44  526.07  125.71  271.89  274.12  373.41    2.33   \n",
      "1     12.73   25.58  439.26  676.48   91.42  110.97  168.78  321.79   67.61   \n",
      "2    306.20  355.49  377.72  462.72   42.92  106.53  545.97  757.45   19.48   \n",
      "3    155.29  238.72  274.79  466.53    9.52   36.11   53.07   65.96   75.63   \n",
      "4      9.20   82.30  152.59  197.13   37.05  105.36  180.67  230.68  115.33   \n",
      "5    122.52  164.22  201.54  218.26   62.69  342.09  377.68  497.12    0.68   \n",
      "6     34.61   41.60   50.97   67.81  267.76  400.21  438.12  445.82   18.99   \n",
      "7    135.06  432.79  503.92  799.80  107.89  215.49  405.02  929.46   61.02   \n",
      "8    155.96  245.41  426.66  706.89   46.77   92.23   92.98  128.60  182.87   \n",
      "9    352.45  388.65  537.12  654.71   38.96   68.38   87.11  173.51  160.33   \n",
      "10   129.75  146.74  281.30  383.85   36.69   54.80  350.68  500.77    2.76   \n",
      "11   180.57  213.19  310.09  430.77   77.47  384.04  571.36  750.60  169.89   \n",
      "12    44.88   49.73  148.15  212.33    1.52    5.00    8.81   12.14   42.06   \n",
      "13    70.23   86.15  118.69  167.98   20.26  159.91  167.14  221.04   86.30   \n",
      "14     4.46    5.35   27.20   38.26  154.47  254.37  258.58  260.56  198.24   \n",
      "15   331.53  398.25  514.71  724.31   52.06  132.49  295.53  641.16    0.95   \n",
      "16    13.90   75.42  333.59  386.60  283.39  504.35  517.62  721.52   13.26   \n",
      "17   125.22  271.99  325.62  561.32    6.57   18.86   19.96   41.89    4.41   \n",
      "18   213.45  311.18  485.38  517.05   19.71   23.51  175.45  258.61  193.73   \n",
      "19   128.63  157.95  172.17  247.47   31.67  239.13  360.55  501.51   62.85   \n",
      "20    13.66   85.88  105.51  252.23   56.23  227.08  412.54  615.53    3.22   \n",
      "21     0.64   54.36   56.12  123.90   48.81  668.50  676.24  888.58  210.41   \n",
      "22     9.78   14.91   20.14   23.04   55.69   95.36   95.68  101.60   68.72   \n",
      "23   236.47  248.14  298.92  371.57   16.29   66.12  179.11  367.37   96.99   \n",
      "24     1.09   90.71  246.87  570.95  203.32  325.75  341.08  479.43   65.21   \n",
      "25   132.65  197.88  474.13  527.01   26.73  151.38  174.22  338.59    1.26   \n",
      "26   151.94  244.72  269.62  344.08    1.26    3.40   20.28   30.96   63.97   \n",
      "27    52.79   82.90   88.56  109.06    5.93  108.72  166.38  245.49   13.38   \n",
      "28    69.22  152.84  237.07  241.89   23.08  129.45  243.97  395.86    0.02   \n",
      "29     8.57   32.44   79.48   86.04    9.67  460.34  492.92  615.22  244.31   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "570   56.48  117.12  244.83  254.89    3.34   11.06   12.62   17.96  289.73   \n",
      "571   19.50   89.65  124.48  150.86  233.23  244.07  244.43  249.07   95.62   \n",
      "572    6.89   10.81   49.88   54.78  225.62  255.16  275.51  395.62   44.03   \n",
      "573  534.55  540.28  703.92  814.51  113.47  143.53  493.51  668.71   15.36   \n",
      "574   29.50  254.66  463.73  597.24  425.32  538.09  708.62  748.61  235.43   \n",
      "575   36.63  189.27  340.27  493.61   47.52   52.75   55.21  112.68   50.09   \n",
      "576  393.03  434.70  530.75  539.67    9.74   24.83  116.59  153.94   48.87   \n",
      "577  181.62  222.63  288.69  295.50  124.67  356.71  381.87  525.62   42.62   \n",
      "578   78.15  114.03  201.03  260.72   79.45  207.64  243.08  346.94    4.20   \n",
      "579    1.26   15.71   58.60  109.60  875.26  921.20  930.61  961.23   27.31   \n",
      "580   21.24   23.68   25.45   26.32   87.28  100.89  110.18  168.23  344.65   \n",
      "581  266.43  437.47  457.42  593.37   55.73   75.45  302.47  387.55   31.77   \n",
      "582  163.49  172.58  499.29  588.29  273.58  383.41  475.17  505.52    3.66   \n",
      "583  145.73  408.27  590.03  619.41  318.72  359.93  411.04  785.83   16.82   \n",
      "584  319.57  320.30  356.22  360.48    2.89   21.86   25.64   60.07   71.49   \n",
      "585   60.67  125.44  169.15  235.38   66.29  171.17  184.92  254.49  189.13   \n",
      "586    7.64   45.39  153.51  159.78   57.42  118.64  143.38  205.23   55.84   \n",
      "587   11.25   74.70   93.52  116.57  610.53  646.07  646.48  703.29  114.06   \n",
      "588  438.14  552.82  573.69  652.63  376.87  446.71  494.08  803.78    5.50   \n",
      "589   16.83  124.73  340.87  489.16   14.65   21.89  104.50  126.44   56.48   \n",
      "590   96.71  358.70  407.69  627.41  141.90  221.78  258.35  276.95    0.47   \n",
      "591  329.28  337.47  530.68  580.28  198.45  228.60  285.76  511.33   12.86   \n",
      "592  123.86  200.21  253.91  292.48   16.84  296.44  306.84  762.80  383.08   \n",
      "593  121.87  181.60  310.41  338.74    0.98    2.27    2.48    3.41   18.32   \n",
      "594   18.84   77.36  112.34  148.60   28.45   46.28   58.19   83.58  279.63   \n",
      "595   34.79   46.14   76.18   77.16    2.62  362.45  381.59  386.28   62.20   \n",
      "596   73.00  268.16  463.22  541.51  241.82  295.17  331.13  575.25   99.69   \n",
      "597  250.48  391.31  621.12  651.81   64.26  112.56  658.79  751.82    6.13   \n",
      "598   27.04  147.63  322.44  342.39   30.28   53.21   58.19   62.89    6.85   \n",
      "599  109.31  256.64  389.21  448.16   94.82  111.79  153.55  256.87   78.45   \n",
      "\n",
      "        9   ...      503     504     505     506     507     508     509  \\\n",
      "0    125.13 ...   280.16   75.74  129.71  367.17  387.05   25.26  125.77   \n",
      "1    133.44 ...   407.39  151.72  279.70  382.50  690.74   19.21  353.76   \n",
      "2    141.69 ...   529.50    4.69    5.81   13.64   22.86  103.76  205.98   \n",
      "3    207.88 ...   636.24   67.78   86.45  167.27  266.47   31.21   90.63   \n",
      "4    166.95 ...    88.90   83.19  190.15  329.11  506.73    7.72   14.52   \n",
      "5     82.43 ...   262.16  216.32  316.88  448.42  629.01  110.10  112.53   \n",
      "6     22.38 ...   512.51   23.41   44.01   56.09  127.96   40.99  230.59   \n",
      "7     86.01 ...   689.92  187.50  257.80  340.29  378.15    4.32   19.13   \n",
      "8    577.14 ...   792.20  212.21  325.24  341.95  347.21  309.19  582.48   \n",
      "9    484.52 ...    93.07   87.79  114.84  519.45  962.47  131.88  205.99   \n",
      "10    68.67 ...   241.78  143.77  168.58  169.61  191.42    9.61   30.86   \n",
      "11   191.74 ...   473.83  142.06  190.19  219.62  325.94  294.57  528.14   \n",
      "12   277.72 ...   483.04   36.37   52.48  531.16  691.98   31.45  150.25   \n",
      "13   180.23 ...   575.59   37.71   58.96   60.11   61.44   77.27   85.97   \n",
      "14   226.16 ...    64.55   13.55  131.57  150.25  268.86  458.43  597.68   \n",
      "15    45.53 ...   207.36    7.68   63.62  401.53  501.79   79.79  168.03   \n",
      "16    41.14 ...   435.59  198.64  607.32  720.42  757.77    7.39  134.20   \n",
      "17    27.71 ...   456.36   61.08   92.39  158.76  170.69  126.20  342.05   \n",
      "18   198.17 ...   879.72   72.96  187.73  431.35  455.76   59.76  289.41   \n",
      "19   515.99 ...    98.71  458.02  631.64  715.05  741.75  120.13  166.16   \n",
      "20   222.60 ...   267.60    8.15   16.61   18.22   31.45  162.06  210.96   \n",
      "21   268.57 ...   463.29    9.84   50.58   64.38  280.76  214.61  449.73   \n",
      "22   247.79 ...   693.74   43.04  326.28  442.67  524.93   68.92   76.48   \n",
      "23   438.88 ...   659.41   99.27  176.46  296.29  500.07    0.26    0.95   \n",
      "24   208.22 ...    70.80   22.56   37.88  123.34  134.73   30.55  125.31   \n",
      "25    29.86 ...   191.98   83.48  212.11  220.22  400.31   48.50  108.64   \n",
      "26    76.90 ...   360.45   77.24  105.55  167.43  442.64    4.13   19.24   \n",
      "27    20.77 ...   684.22  274.43  309.55  685.10  782.97   21.52  212.66   \n",
      "28     0.10 ...   836.19    0.21   74.42   82.84  115.50   57.79  184.41   \n",
      "29   299.78 ...    88.76   52.19  194.51  367.30  470.61   11.84   74.35   \n",
      "..      ... ...      ...     ...     ...     ...     ...     ...     ...   \n",
      "570  304.71 ...   234.91    0.18    2.26    2.70    2.85   42.96   47.52   \n",
      "571  187.83 ...   405.78   27.63  105.74  128.91  169.52    9.63  195.01   \n",
      "572  327.22 ...   603.71   13.89   26.64   78.12  160.76  186.87  346.23   \n",
      "573  181.30 ...   929.68  151.66  533.68  564.48  814.35   38.97   60.25   \n",
      "574  276.91 ...   143.30   25.80   26.27   37.44   76.19   96.46  206.88   \n",
      "575  151.90 ...   319.71  104.02  134.63  321.62  383.07  191.38  325.85   \n",
      "576   81.89 ...   477.40   17.63  205.28  272.48  417.36   14.70   58.25   \n",
      "577   60.15 ...   616.38  256.32  329.58  675.89  945.55  393.72  427.34   \n",
      "578   66.45 ...   736.63   88.84   90.57  133.68  210.27  282.93  307.92   \n",
      "579   34.55 ...   138.44   40.78   87.94  217.81  287.81   41.54   71.32   \n",
      "580  805.14 ...   235.93  170.02  369.83  599.07  600.91   59.53  385.68   \n",
      "581  136.67 ...   346.98   22.39   28.60   32.58   55.03   94.79  219.46   \n",
      "582  203.43 ...   439.30   46.01  175.83  189.27  321.85   10.10   35.20   \n",
      "583  333.43 ...   658.83  101.29  329.24  369.01  508.01  261.48  616.56   \n",
      "584  257.51 ...   125.56  108.15  452.78  571.80  729.68  127.23  141.50   \n",
      "585  345.57 ...   234.82   33.32   81.41  119.65  125.27   53.96  227.42   \n",
      "586   72.24 ...   408.73   50.13  136.05  327.11  397.20  122.79  237.15   \n",
      "587  201.48 ...   609.71  409.16  556.44  557.61  651.28   49.05   75.85   \n",
      "588   78.42 ...   837.77    2.71    4.55    5.33    8.95    7.75   24.74   \n",
      "589   77.87 ...   133.09   16.53   19.50  244.13  268.47   13.04   18.47   \n",
      "590   14.90 ...    72.87  347.63  372.80  400.78  502.35  294.35  648.32   \n",
      "591  181.80 ...   101.10  208.23  251.36  300.30  783.62  126.88  230.43   \n",
      "592  483.90 ...   275.75    0.50  103.66  120.07  121.81   15.94   40.73   \n",
      "593   95.55 ...   524.61    3.73  170.96  262.67  268.18    8.56  111.40   \n",
      "594  398.37 ...   131.57    9.79   34.49  205.46  239.11  187.41  245.56   \n",
      "595  135.32 ...   328.42   34.61   88.29  578.86  911.63    4.12   10.56   \n",
      "596  105.32 ...   498.89   31.63   45.78  142.14  170.81  318.23  499.87   \n",
      "597  132.19 ...   650.64   90.11  128.17  208.94  403.17   73.63  129.67   \n",
      "598  215.07 ...   783.67   71.63  171.64  295.13  712.28   39.71   92.00   \n",
      "599   86.14 ...   104.90   18.46   39.16   55.52   68.87  186.64  396.74   \n",
      "\n",
      "        510     511  512  \n",
      "0    149.49  284.87    1  \n",
      "1    492.37  534.29    0  \n",
      "2    372.56  613.87    2  \n",
      "3    116.32  273.30    2  \n",
      "4     23.53   28.09    0  \n",
      "5    113.44  719.24    3  \n",
      "6    247.00  278.04    2  \n",
      "7     19.42   84.09    0  \n",
      "8    647.02  767.30    1  \n",
      "9    368.06  426.86    1  \n",
      "10   109.56  118.86    1  \n",
      "11   560.09  689.74    1  \n",
      "12   395.54  445.43    1  \n",
      "13   133.77  164.80    2  \n",
      "14   770.29  831.23    1  \n",
      "15   271.39  349.72    1  \n",
      "16   164.29  172.84    0  \n",
      "17   491.13  515.44    0  \n",
      "18   327.83  586.51    1  \n",
      "19   174.25  265.82    2  \n",
      "20   367.47  370.09    0  \n",
      "21   510.83  533.91    0  \n",
      "22    91.56  144.76    2  \n",
      "23     2.56    3.13    2  \n",
      "24   210.22  466.59    0  \n",
      "25   169.56  310.06    1  \n",
      "26    44.92   46.70    2  \n",
      "27   384.55  666.91    2  \n",
      "28   293.40  407.18    0  \n",
      "29    91.47   96.42    0  \n",
      "..      ...     ...  ...  \n",
      "570   52.06   67.64    0  \n",
      "571  208.15  671.32    0  \n",
      "572  366.39  389.03    1  \n",
      "573   93.89  100.46    2  \n",
      "574  590.60  690.32    0  \n",
      "575  331.91  355.83    0  \n",
      "576  105.65  154.75    2  \n",
      "577  495.38  818.95    3  \n",
      "578  488.04  530.28    1  \n",
      "579  102.91  113.77    0  \n",
      "580  671.01  725.12    3  \n",
      "581  245.28  347.55    2  \n",
      "582   85.69  151.14    1  \n",
      "583  631.25  937.88    0  \n",
      "584  167.93  256.09    3  \n",
      "585  254.17  272.31    0  \n",
      "586  269.18  493.29    0  \n",
      "587  440.60  542.56    0  \n",
      "588  120.96  258.74    3  \n",
      "589   33.31   36.99    0  \n",
      "590  662.64  689.59    0  \n",
      "591  350.01  379.89    1  \n",
      "592   62.34   69.82    3  \n",
      "593  588.50  615.37    1  \n",
      "594  252.24  322.11    0  \n",
      "595   88.15  129.42    3  \n",
      "596  500.31  526.42    0  \n",
      "597  279.71  400.79    0  \n",
      "598  122.80  157.57    0  \n",
      "599  422.76  548.26    0  \n",
      "\n",
      "[600 rows x 513 columns]\n",
      "        0       1       2       3       4       5       6       7       8    \\\n",
      "0    180.80  242.71  495.44  526.07  125.71  271.89  274.12  373.41    2.33   \n",
      "1     12.73   25.58  439.26  676.48   91.42  110.97  168.78  321.79   67.61   \n",
      "2    306.20  355.49  377.72  462.72   42.92  106.53  545.97  757.45   19.48   \n",
      "3    155.29  238.72  274.79  466.53    9.52   36.11   53.07   65.96   75.63   \n",
      "4      9.20   82.30  152.59  197.13   37.05  105.36  180.67  230.68  115.33   \n",
      "5    122.52  164.22  201.54  218.26   62.69  342.09  377.68  497.12    0.68   \n",
      "6     34.61   41.60   50.97   67.81  267.76  400.21  438.12  445.82   18.99   \n",
      "7    135.06  432.79  503.92  799.80  107.89  215.49  405.02  929.46   61.02   \n",
      "8    155.96  245.41  426.66  706.89   46.77   92.23   92.98  128.60  182.87   \n",
      "9    352.45  388.65  537.12  654.71   38.96   68.38   87.11  173.51  160.33   \n",
      "10   129.75  146.74  281.30  383.85   36.69   54.80  350.68  500.77    2.76   \n",
      "11   180.57  213.19  310.09  430.77   77.47  384.04  571.36  750.60  169.89   \n",
      "12    44.88   49.73  148.15  212.33    1.52    5.00    8.81   12.14   42.06   \n",
      "13    70.23   86.15  118.69  167.98   20.26  159.91  167.14  221.04   86.30   \n",
      "14     4.46    5.35   27.20   38.26  154.47  254.37  258.58  260.56  198.24   \n",
      "15   331.53  398.25  514.71  724.31   52.06  132.49  295.53  641.16    0.95   \n",
      "16    13.90   75.42  333.59  386.60  283.39  504.35  517.62  721.52   13.26   \n",
      "17   125.22  271.99  325.62  561.32    6.57   18.86   19.96   41.89    4.41   \n",
      "18   213.45  311.18  485.38  517.05   19.71   23.51  175.45  258.61  193.73   \n",
      "19   128.63  157.95  172.17  247.47   31.67  239.13  360.55  501.51   62.85   \n",
      "20    13.66   85.88  105.51  252.23   56.23  227.08  412.54  615.53    3.22   \n",
      "21     0.64   54.36   56.12  123.90   48.81  668.50  676.24  888.58  210.41   \n",
      "22     9.78   14.91   20.14   23.04   55.69   95.36   95.68  101.60   68.72   \n",
      "23   236.47  248.14  298.92  371.57   16.29   66.12  179.11  367.37   96.99   \n",
      "24     1.09   90.71  246.87  570.95  203.32  325.75  341.08  479.43   65.21   \n",
      "25   132.65  197.88  474.13  527.01   26.73  151.38  174.22  338.59    1.26   \n",
      "26   151.94  244.72  269.62  344.08    1.26    3.40   20.28   30.96   63.97   \n",
      "27    52.79   82.90   88.56  109.06    5.93  108.72  166.38  245.49   13.38   \n",
      "28    69.22  152.84  237.07  241.89   23.08  129.45  243.97  395.86    0.02   \n",
      "29     8.57   32.44   79.48   86.04    9.67  460.34  492.92  615.22  244.31   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "570   56.48  117.12  244.83  254.89    3.34   11.06   12.62   17.96  289.73   \n",
      "571   19.50   89.65  124.48  150.86  233.23  244.07  244.43  249.07   95.62   \n",
      "572    6.89   10.81   49.88   54.78  225.62  255.16  275.51  395.62   44.03   \n",
      "573  534.55  540.28  703.92  814.51  113.47  143.53  493.51  668.71   15.36   \n",
      "574   29.50  254.66  463.73  597.24  425.32  538.09  708.62  748.61  235.43   \n",
      "575   36.63  189.27  340.27  493.61   47.52   52.75   55.21  112.68   50.09   \n",
      "576  393.03  434.70  530.75  539.67    9.74   24.83  116.59  153.94   48.87   \n",
      "577  181.62  222.63  288.69  295.50  124.67  356.71  381.87  525.62   42.62   \n",
      "578   78.15  114.03  201.03  260.72   79.45  207.64  243.08  346.94    4.20   \n",
      "579    1.26   15.71   58.60  109.60  875.26  921.20  930.61  961.23   27.31   \n",
      "580   21.24   23.68   25.45   26.32   87.28  100.89  110.18  168.23  344.65   \n",
      "581  266.43  437.47  457.42  593.37   55.73   75.45  302.47  387.55   31.77   \n",
      "582  163.49  172.58  499.29  588.29  273.58  383.41  475.17  505.52    3.66   \n",
      "583  145.73  408.27  590.03  619.41  318.72  359.93  411.04  785.83   16.82   \n",
      "584  319.57  320.30  356.22  360.48    2.89   21.86   25.64   60.07   71.49   \n",
      "585   60.67  125.44  169.15  235.38   66.29  171.17  184.92  254.49  189.13   \n",
      "586    7.64   45.39  153.51  159.78   57.42  118.64  143.38  205.23   55.84   \n",
      "587   11.25   74.70   93.52  116.57  610.53  646.07  646.48  703.29  114.06   \n",
      "588  438.14  552.82  573.69  652.63  376.87  446.71  494.08  803.78    5.50   \n",
      "589   16.83  124.73  340.87  489.16   14.65   21.89  104.50  126.44   56.48   \n",
      "590   96.71  358.70  407.69  627.41  141.90  221.78  258.35  276.95    0.47   \n",
      "591  329.28  337.47  530.68  580.28  198.45  228.60  285.76  511.33   12.86   \n",
      "592  123.86  200.21  253.91  292.48   16.84  296.44  306.84  762.80  383.08   \n",
      "593  121.87  181.60  310.41  338.74    0.98    2.27    2.48    3.41   18.32   \n",
      "594   18.84   77.36  112.34  148.60   28.45   46.28   58.19   83.58  279.63   \n",
      "595   34.79   46.14   76.18   77.16    2.62  362.45  381.59  386.28   62.20   \n",
      "596   73.00  268.16  463.22  541.51  241.82  295.17  331.13  575.25   99.69   \n",
      "597  250.48  391.31  621.12  651.81   64.26  112.56  658.79  751.82    6.13   \n",
      "598   27.04  147.63  322.44  342.39   30.28   53.21   58.19   62.89    6.85   \n",
      "599  109.31  256.64  389.21  448.16   94.82  111.79  153.55  256.87   78.45   \n",
      "\n",
      "        9     ...       118     119     120     121     122     123     124  \\\n",
      "0    125.13   ...    704.50  831.82   44.25   83.42   85.17  150.32  145.41   \n",
      "1    133.44   ...    312.18  611.54   97.77  160.13  211.29  232.36   31.44   \n",
      "2    141.69   ...    267.53  352.77   43.01  103.30  145.40  167.57    8.29   \n",
      "3    207.88   ...     42.93   50.64   48.76  100.01  106.15  197.88  398.99   \n",
      "4    166.95   ...    398.52  523.60   42.38   87.91  367.83  403.76  305.72   \n",
      "5     82.43   ...    196.29  301.75   66.99   95.49  214.25  519.23  120.60   \n",
      "6     22.38   ...    135.06  137.29    6.65   37.26  330.13  479.65    4.23   \n",
      "7     86.01   ...    581.05  774.16  415.54  456.70  615.77  710.36  175.73   \n",
      "8    577.14   ...    324.49  355.13  167.51  214.33  326.91  736.25  159.10   \n",
      "9    484.52   ...     95.87  207.09  213.75  481.91  753.31  776.11   12.85   \n",
      "10    68.67   ...    451.46  791.67   76.07  498.96  761.56  847.32    7.56   \n",
      "11   191.74   ...    357.99  553.10    0.78    1.20    3.53    4.26   14.23   \n",
      "12   277.72   ...    199.09  265.40   47.22   54.33   58.19   62.20   18.50   \n",
      "13   180.23   ...      1.81    3.18   64.95   87.59   96.31  144.24  104.86   \n",
      "14   226.16   ...    520.48  522.82   13.69   67.84   92.46  129.47    9.65   \n",
      "15    45.53   ...    318.72  344.47   72.81  153.82  193.62  206.08  394.98   \n",
      "16    41.14   ...     55.47   78.48   28.63  120.04  169.35  370.40   16.74   \n",
      "17    27.71   ...    715.74  732.83  154.02  176.07  404.10  424.16   50.82   \n",
      "18   198.17   ...    347.58  478.74  173.97  303.35  359.56  384.12   14.87   \n",
      "19   515.99   ...     62.24  148.27   25.43  128.53  163.05  339.61  238.72   \n",
      "20   222.60   ...    300.65  510.88  143.32  259.73  270.58  563.62  289.91   \n",
      "21   268.57   ...    371.46  451.21  114.77  177.02  354.02  545.03   42.62   \n",
      "22   247.79   ...    135.46  252.68   58.93   70.91   77.40   90.41    0.65   \n",
      "23   438.88   ...    298.29  779.84   31.22  384.97  656.47  800.00   76.26   \n",
      "24   208.22   ...    383.85  626.43  358.08  706.11  750.04  933.15   90.83   \n",
      "25    29.86   ...    236.79  312.13    0.17    3.98   17.22   27.62   63.44   \n",
      "26    76.90   ...     16.97   21.04   41.69   49.66   65.87   88.42    2.81   \n",
      "27    20.77   ...    416.04  580.15   41.64   47.38  140.40  146.34   75.41   \n",
      "28     0.10   ...    360.10  366.21   17.06   44.09  183.57  224.82   26.33   \n",
      "29   299.78   ...     72.49   88.06  169.53  171.55  217.92  291.14  124.35   \n",
      "..      ...   ...       ...     ...     ...     ...     ...     ...     ...   \n",
      "570  304.71   ...     46.56   76.95   73.76  144.11  230.03  323.95   10.79   \n",
      "571  187.83   ...    494.38  672.89   67.76  342.57  354.25  533.64   12.70   \n",
      "572  327.22   ...    195.82  416.44    3.74  352.37  463.12  503.85    7.99   \n",
      "573  181.30   ...     82.40  105.34  242.63  266.08  353.35  449.53  155.54   \n",
      "574  276.91   ...    688.68  690.74   15.02  345.52  370.68  532.53   73.01   \n",
      "575  151.90   ...    291.28  337.20   42.03  267.31  328.87  541.36   21.18   \n",
      "576   81.89   ...    169.84  216.81  139.41  255.52  303.40  360.69    0.40   \n",
      "577   60.15   ...    692.99  915.21  269.12  730.35  893.59  964.89  295.00   \n",
      "578   66.45   ...    441.84  510.98    4.86   11.00   27.89   41.92  208.86   \n",
      "579   34.55   ...    138.30  291.28   19.26   21.95   55.13   81.10  191.94   \n",
      "580  805.14   ...     29.60   33.10   18.71   95.76  156.41  159.66   10.23   \n",
      "581  136.67   ...    711.43  717.16    9.34   37.78   85.89  118.95  108.61   \n",
      "582  203.43   ...    130.71  243.05   24.22   51.50  167.81  335.33   40.74   \n",
      "583  333.43   ...     35.44   72.33  292.90  348.86  381.58  410.00  142.84   \n",
      "584  257.51   ...    647.18  739.83   90.23  163.40  399.80  419.01    0.71   \n",
      "585  345.57   ...    274.04  375.92  300.03  365.08  447.63  543.81   75.89   \n",
      "586   72.24   ...     94.39  109.31  305.83  379.54  467.43  560.83   52.69   \n",
      "587  201.48   ...    324.40  756.75  161.25  162.21  222.05  478.41   53.72   \n",
      "588   78.42   ...    456.49  536.88  113.75  451.78  480.58  672.63    5.54   \n",
      "589   77.87   ...    148.36  181.45  419.57  473.93  792.34  837.85   34.33   \n",
      "590   14.90   ...    542.23  762.19  112.50  253.73  458.47  753.75   76.18   \n",
      "591  181.80   ...    418.03  678.08    1.49    2.13    2.21    2.58   16.45   \n",
      "592  483.90   ...    278.23  296.61   28.57   29.42   36.46   74.32    0.21   \n",
      "593   95.55   ...     51.68   54.17    2.23   46.19   46.83   47.56  338.39   \n",
      "594  398.37   ...    174.58  615.94   31.40   56.28  144.23  148.50   83.11   \n",
      "595  135.32   ...    439.88  447.23    4.01   56.91   82.28  288.51  146.83   \n",
      "596  105.32   ...    112.40  147.41   48.73  133.71  297.92  336.00   13.81   \n",
      "597  132.19   ...    809.94  846.24   31.73  239.27  282.79  314.41    3.09   \n",
      "598  215.07   ...    106.11  142.42    5.91   29.92   88.53  243.14   39.05   \n",
      "599   86.14   ...    203.93  228.81  178.96  239.38  473.23  560.73   36.76   \n",
      "\n",
      "        125     126     127  \n",
      "0    214.68  458.36  535.78  \n",
      "1    149.41  229.07  242.67  \n",
      "2     25.52   45.90   58.03  \n",
      "3    432.15  489.61  491.11  \n",
      "4    386.67  390.70  548.99  \n",
      "5    152.88  243.98  284.95  \n",
      "6     31.08   61.14   63.05  \n",
      "7    203.79  280.14  733.11  \n",
      "8    251.26  364.13  551.25  \n",
      "9     19.57   69.11  201.27  \n",
      "10    10.16   31.67   53.80  \n",
      "11    76.70  155.54  738.89  \n",
      "12   117.46  149.32  186.71  \n",
      "13   123.22  232.00  297.37  \n",
      "14    22.14   31.52   48.20  \n",
      "15   485.09  630.18  679.15  \n",
      "16   311.33  430.74  536.10  \n",
      "17   133.59  189.44  213.37  \n",
      "18    27.05   29.30   35.75  \n",
      "19   439.62  475.48  626.99  \n",
      "20   388.75  401.30  465.93  \n",
      "21   135.39  265.52  266.61  \n",
      "22     3.86   11.92   16.35  \n",
      "23   254.69  277.83  775.14  \n",
      "24   155.11  221.39  272.14  \n",
      "25    67.16  192.12  228.30  \n",
      "26     4.39    5.24   14.62  \n",
      "27    86.20  305.71  531.39  \n",
      "28    61.48  148.61  242.86  \n",
      "29   139.95  141.76  254.43  \n",
      "..      ...     ...     ...  \n",
      "570  215.86  554.85  567.43  \n",
      "571   68.18  101.18  132.22  \n",
      "572   17.55   28.09   58.01  \n",
      "573  155.89  210.77  487.03  \n",
      "574  149.91  384.42  444.58  \n",
      "575   33.04  106.03  304.66  \n",
      "576    9.98   23.90   30.40  \n",
      "577  396.30  760.43  767.85  \n",
      "578  308.31  313.24  487.33  \n",
      "579  224.15  239.39  245.28  \n",
      "580   32.39   34.28   42.52  \n",
      "581  515.90  554.51  629.64  \n",
      "582  177.68  344.26  469.16  \n",
      "583  149.28  165.06  270.43  \n",
      "584   16.90   23.41   30.25  \n",
      "585  271.39  354.61  707.53  \n",
      "586   91.05  207.21  406.67  \n",
      "587   67.97   80.05   90.85  \n",
      "588    6.63    9.93   12.70  \n",
      "589  160.74  306.66  520.14  \n",
      "590  241.17  244.40  447.66  \n",
      "591   22.67  143.89  233.32  \n",
      "592    2.09    2.60    5.13  \n",
      "593  530.02  559.84  728.01  \n",
      "594  321.67  390.33  453.23  \n",
      "595  207.48  209.50  212.40  \n",
      "596   25.76  776.88  924.81  \n",
      "597  162.28  325.39  533.91  \n",
      "598  201.32  321.98  422.29  \n",
      "599   67.79  141.09  142.89  \n",
      "\n",
      "[600 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "# No threading version.\n",
    "start = time.time()\n",
    "# Each index corresponds to an instance.\n",
    "makespan_instance_machines_heuristic = []\n",
    "makespan_instance_machines_prediction = []\n",
    "SCALE_DATA = True\n",
    "USE_PARAMETER_SELECTION = False\n",
    "# Within each index, there'll be an array of machine_amount elements, in which each element\n",
    "# is the time during which each machine is running\n",
    "# Something along the lines of [[10,20,9,40], [99,88,22,11], ..., [10,9,21,35]]\n",
    "for i in range(0, 1): # For each task/classifier\n",
    "    print(\"Training classifier \" + str(i) + \"...\")\n",
    "    # Data is loaded.\n",
    "    TRAINING_FILE = baseDir + 'training/' + str(i) + '.csv' # Training file for current classifier\n",
    "    TEST_FILE = baseDir + 'test/' + str(i) + '.csv' # Test file for current classifier\n",
    "    training_set = pd.read_csv(TRAINING_FILE, header=None, delimiter=',')\n",
    "    test_set = pd.read_csv(TEST_FILE, header=None, delimiter=',')\n",
    "    \n",
    "    # Create dataframe for data and separate target.\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    print(df_training)\n",
    "    df_training_input = df_training.iloc[:, :-1] \n",
    "    print(df_training_input.iloc[:, i * task_amount : i * task_amount + task_amount])\n",
    "    df_training_output = df_training.iloc[:, -1]\n",
    "    \n",
    "    # Validation/testing data is loaded.\n",
    "    df_test = pd.DataFrame(test_set)\n",
    "    df_test_input = df_test.iloc[:, :-1]\n",
    "    df_test_output = df_test.iloc[:, -1]\n",
    "    \n",
    "#     if SCALE_DATA:\n",
    "#         # Scale data because http://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use\n",
    "#         scaler = StandardScaler()  \n",
    "#         # Fit only on training data.\n",
    "#         scaler.fit(df_training_input)\n",
    "#         # Reconvert input training data to dataframe after scaling (which converts it to an array of arrays).\n",
    "#         df_training_input = pd.DataFrame(scaler.transform(df_training_input))\n",
    "#         # Re-init scaler just in case.\n",
    "#         scaler = StandardScaler()  \n",
    "#         scaler.fit(df_test_input)\n",
    "#         # Scale test data.\n",
    "#         df_test_input = pd.DataFrame(scaler.transform(df_test_input))        \n",
    "#     if current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "#         if USE_PARAMETER_SELECTION:\n",
    "#             # Grid of parameters, including all posible parameters for each configuration of\n",
    "#             # an SVM classifier.\n",
    "#             param_grid = [\n",
    "#               {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    "#              ]\n",
    "#             # Run grid search with all the possible classifier configurations.\n",
    "#             classifiers[i] = GridSearchCV(classifiers[i], param_grid=param_grid)\n",
    "#             # This generates multiple estimators.\n",
    "#             # Now the prediction will use the best estimator of all.\n",
    "#             # Should use grid_search as new classifier, persist it, and use it for prediction\n",
    "#             # as a normal classifier (according to documentation it uses the best estimator)\n",
    "#             # However, it fits every possible estimator with the data, so that's something of note.            \n",
    "#     # Classifier is trained using the data.\n",
    "#     classifiers[i].fit(df_training_input, df_training_output)\n",
    "#     # Classifier directory is generated if it doesn't exist.\n",
    "#     generar_jobs.generate_dir(model_base_path)\n",
    "#     # Classifier is persisted.\n",
    "#     joblib.dump(classifiers[i], model_base_path + model_file_prefix + str(i) \\\n",
    "#                 + model_file_extension)\n",
    "#     # Classifier accuracy is determined using test data.\n",
    "#     results = []\n",
    "#     # Go through every test instance manually to calculate makespan for each\n",
    "#     # problem-classifier/task pair\n",
    "#     current_task_index = i * machine_amount # Column index within etc matrix\n",
    "#     print(\"    Doing makespan stuff...\")\n",
    "#     test_instance_amount = len(df_test)\n",
    "#     for j in range(0, test_instance_amount): # For every validation instance\n",
    "#         # df_test.iloc[j] is an ETC matrix + the corresponding classification for one task\n",
    "#         etc_matrix_scaled = df_test_input.iloc[j] # Scaled data for classification (since classifiers were\n",
    "#         # trained using scaled data)\n",
    "#         # Non-scaled data is used to calculate real makespan, using the original units of the problem.\n",
    "#         etc_matrix = df_test.iloc[j][:-1] # Get j problem instance, ignoring last column (the output/classification).\n",
    "#         classification_heuristic = float(df_test_output[j])\n",
    "#         # Every test example is classified, and its classification is appended\n",
    "#         # to a results array.\n",
    "#         # Make prediction for current problem instance or etc matrix (using scaled data).\n",
    "#         prediction_pandas = float(classifiers[i].predict(etc_matrix_scaled.values.reshape(1, -1)))\n",
    "#         results.append(prediction_pandas)\n",
    "#         prediction = float(prediction_pandas) # To work in floats.\n",
    "\n",
    "#         # Get subrow from original input data, to get the task/machine times right.\n",
    "#         sub_row_for_current_task = etc_matrix[current_task_index:current_task_index + machine_amount]\n",
    "#         # Makespan value for prediction\n",
    "#         current_makespan_prediction = sub_row_for_current_task[current_task_index + prediction]\n",
    "#         # Makespan value for heuristic\n",
    "#         current_makespan_heuristic = sub_row_for_current_task[current_task_index + classification_heuristic]\n",
    "#         if len(makespan_instance_machines_prediction) <= j: # If there's no entry for this problem instance.\n",
    "#             # Init entry for problem instance, with each machine's makespan starting at 0.0.\n",
    "#             makespan_instance_machines_prediction.append([0.0] * machine_amount)\n",
    "#             makespan_instance_machines_heuristic.append([0.0] * machine_amount)\n",
    "#         makespan_instance_machines_prediction[j][int(prediction)] += current_makespan_prediction\n",
    "#         makespan_instance_machines_heuristic[j][int(classification_heuristic)] += current_makespan_heuristic\n",
    "#     print(\"    Done with makespan stuff...\")\n",
    "#     # Actual classification results are compared to expected values.\n",
    "#     accuracy = accuracy_score(df_test_output, results)\n",
    "#     print(\"    Classifier accuracy: \" + str(accuracy))\n",
    "#     # Calculated accuracy is added to accuracies list.\n",
    "#     accuracy_scores.append(accuracy)\n",
    "# end = time.time()\n",
    "# print('The execution took ' + str(end - start) + ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
