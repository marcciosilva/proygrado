{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import parser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import generar_jobs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem parameters.\n",
    "# TODO receive as parameters\n",
    "task_amount = 128\n",
    "machine_amount = 4\n",
    "task_heterogeneity = 0\n",
    "machine_heterogeneity = 0\n",
    "consistency_type = 0\n",
    "accuracy_scores = []\n",
    "classifiers = []\n",
    "# Makespan stuff\n",
    "# Preciso un vector donde para cada indice tengo el makespan de una instancia de validacion (voy a tener\n",
    "# tantas entradas como instancias de validacion tenga)\n",
    "# En definitiva van a ser dos vectores, uno para los makespan por tarea de la heuristica, y otro para\n",
    "# mis clasificadores\n",
    "makespan_per_machine_heuristic = [[]] * machine_amount\n",
    "makespan_per_machine_prediction = [[]] * machine_amount\n",
    "# Classifier configuration.\n",
    "CLASSIFIER_STRING_ANN = 'ann'\n",
    "CLASSIFIER_STRING_SVM = 'svm'\n",
    "classifier_types = [CLASSIFIER_STRING_ANN, CLASSIFIER_STRING_SVM]\n",
    "current_classifier_index = 0 # Only modify this.\n",
    "current_classifier_str = classifier_types[current_classifier_index]\n",
    "# Base path for classifier persistence.\n",
    "model_base_path = './models/' + current_classifier_str + '/' + str(task_amount) + 'x' + str(machine_amount) \\\n",
    "    + '-' + str(task_heterogeneity) + str(machine_heterogeneity) \\\n",
    "    + str(consistency_type) + '/'\n",
    "baseDir = './data-processed/' + str(task_amount) + 'x' \\\n",
    "    + str(machine_amount) + '-' + str(task_heterogeneity) \\\n",
    "    + str(machine_heterogeneity) + str(consistency_type) + '/'\n",
    "model_file_prefix = 'clf-' + current_classifier_str\n",
    "model_file_extension = '.pkl'\n",
    "\n",
    "if current_classifier_str == CLASSIFIER_STRING_ANN:\n",
    "    dimension = task_amount * machine_amount\n",
    "    # Reference: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "    ns = 600  # Amount of training examples.\n",
    "    ni = dimension\n",
    "    no = 1  # Amount of output neurons.\n",
    "    alpha = 2\n",
    "    hidden_layer_amount = int(math.ceil(ns / (alpha * (ni + no))))\n",
    "    # Each hidden layer has an intermediate amount of neurons (between the neuron amount\n",
    "    # present in the output layer and the input layer).\n",
    "    # A tuple is generated to set up the MLPClassifier.\n",
    "    hidden_layer_neuron_amount = tuple([int(math.ceil((task_amount - no) / 2))]\n",
    "                                       * hidden_layer_amount)  \n",
    "elif current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "    # No config necessary for SVC method.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load the classifiers (if they already exist).\n",
    "# TODO maybe specify classifier configuration along with this (so as to not specify something that might already exist)\n",
    "for i in range(0, task_amount):\n",
    "    try:\n",
    "        classifier = joblib.load(model_base_path + model_file_prefix + str(i) \\\n",
    "                                 + model_file_extension)\n",
    "    except Exception:\n",
    "        print('The classifier for output ' + str(i) + ' didn\\'t exist.')\n",
    "        if current_classifier_str == CLASSIFIER_STRING_ANN:\n",
    "            classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, \n",
    "                hidden_layer_sizes=hidden_layer_neuron_amount, random_state=1)\n",
    "        elif current_classifier_str == CLASSIFIER_STRING_SVM:\n",
    "            classifier = svm.SVC()\n",
    "    finally:\n",
    "        # Append classifier to classifier list (in memory).\n",
    "        classifiers.append(classifier)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena cada clasificador, y para cada uno se hace lo siguiente (post-entrenamiento)\n",
    "* Se evalua la accuracy usando el training set\n",
    "* Se recorre cada instancia del training set y se calcula el makespan que aporta el clasificador correspondiente\n",
    "    * O sea que se va a obtener un vector, donde cada entrada es el makespan para una instancia de entrenamiento distinta\n",
    "    * Como este vector eventualmente se va a obtener para cada clasificador, se va a tener una matriz, donde el primer indice accede a un clasificador, y el segundo a un makespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No threading version\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, task_amount): # For each task/classifier\n",
    "    print(\"Training classifier \" + str(i) + \"...\")\n",
    "    # Data is loaded.\n",
    "    TRAINING_FILE = baseDir + 'training/' + str(i) + '.csv' # Training file for current classifier\n",
    "    TEST_FILE = baseDir + 'test/' + str(i) + '.csv' # Test file for current classifier\n",
    "    training_set = pd.read_csv(TRAINING_FILE, header=None, delimiter=',')\n",
    "    test_set = pd.read_csv(TEST_FILE, header=None, delimiter=',')\n",
    "    # Create dataframe for data and separate target.\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    y_training = df_training.iloc[:, -1]\n",
    "    # Validation/testing data is loaded.\n",
    "    df_test = pd.DataFrame(test_set)\n",
    "    y_test = df_test.iloc[:, -1]\n",
    "    # Classifier is trained using the data.\n",
    "    classifiers[i].fit(df_training.iloc[:, :-1], y_training)\n",
    "    # Classifier directory is generated if it doesn't exist.\n",
    "    generar_jobs.generate_dir(model_base_path)\n",
    "    # Classifier is persisted.\n",
    "    joblib.dump(classifiers[i], model_base_path + model_file_prefix + str(i) \\\n",
    "                + model_file_extension)\n",
    "    # Classifier accuracy is determined using test data.\n",
    "    results = []\n",
    "    # Go through every test instance manually to calculate makespan for each\n",
    "    # problem-classifier/task pair\n",
    "    current_task_index = i * machine_amount # Column index within etc matrix\n",
    "    print(\"    Doing makespan stuff...\")\n",
    "    test_instance_amount = len(df_test)\n",
    "    current_task_makespan_prediction = [[]] * machine_amount # Array that holds makespan for every validation instance, \n",
    "    # for only one task\n",
    "    current_task_makespan_heuristic = [[]] * machine_amount # Array that holds makespan for every validation instance, \n",
    "    # for only one task\n",
    "    for j in range(0, test_instance_amount): # For every validation instance\n",
    "        # df_test.iloc[j] is an ETC matrix + the corresponding classification for one task\n",
    "        problem_instance = df_test.iloc[j]\n",
    "        etc_matrix = problem_instance[:-1]\n",
    "        classification_heuristic = float(problem_instance[-1:]) # float format\n",
    "        # Every test example is classified, and its classification is appended\n",
    "        # to a results array.\n",
    "        # Make prediction for current problem instance or etc matrix\n",
    "        prediction_pandas = float(classifiers[i].predict(etc_matrix.values.reshape(1, -1)))\n",
    "        results.append(prediction_pandas)\n",
    "        prediction = float(prediction_pandas) # To work in floats\n",
    "        # TODO Agregar resultados de clasificacion a un array, para comparar con y_test\n",
    "        # asi obtengo medida de accuracy aparte de esto que estoy armando\n",
    "        # Para cada ejemplo de validacion, determino cuanto tiempo aporta al makespan de la tarea/clasificador actual\n",
    "        # Get times for all machinesi\n",
    "        sub_row_for_current_task = etc_matrix[current_task_index:current_task_index + machine_amount]\n",
    "        # Makespan value for prediction\n",
    "        current_makespan_prediction = sub_row_for_current_task[current_task_index + prediction]\n",
    "        # Makespan value for heuristic\n",
    "        current_makespan_heuristic = sub_row_for_current_task[current_task_index + classification_heuristic]        \n",
    "#         print('    Sub row for current task is: \\n' + str(sub_row_for_current_task))\n",
    "#         print('        Prediction is: ' + str(prediction) + ', Makespan is: ' + str(current_makespan_prediction))\n",
    "#         print('        Heuristic is: ' + str(classification_heuristic) + ', Makespan is: ' + str(current_makespan_heuristic))\n",
    "        current_task_makespan_prediction[int(prediction)].append(current_makespan_prediction)\n",
    "        current_task_makespan_heuristic[int(classification_heuristic)].append(current_makespan_heuristic)\n",
    "    # Cada indice de current_task_makespan_* tiene el vector de makespan values para una instancia en una maquina\n",
    "    for j in range(0, machine_amount):\n",
    "        makespan_per_machine_prediction[j].append([]) # Append new array for current task\n",
    "        makespan_per_machine_heuristic[j].append([]) # Append new array for current task        \n",
    "        makespan_per_machine_prediction[j][-1].append(current_task_makespan_prediction[j])\n",
    "        makespan_per_machine_heuristic[j][-1].append(current_task_makespan_heuristic[j])        \n",
    "    # Actual results are compared to expected values.\n",
    "    accuracy = accuracy_score(y_test, results)\n",
    "    print(\"    Classifier accuracy: \" + str(accuracy))\n",
    "    # Calculated accuracy is added to accuracies list.\n",
    "    accuracy_scores.append(accuracy)\n",
    "end = time.time()\n",
    "print('The execution took ' + str(end - start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(makespan_for_instances_heuristic))\n",
    "makespan_differences = [] # Array that holds, for each validation instance, the difference between\n",
    "# heuristic and prediction\n",
    "validation_instance_amount = len(makespan_for_instances_prediction[0])\n",
    "for current_instance in range(0, validation_instance_amount):\n",
    "    print(\"Instance \" + str(current_instance) + \":\")\n",
    "    current_makespan_heuristic = 0.0\n",
    "    for task in range(0, task_amount):\n",
    "        current_makespan_heuristic += makespan_for_instances_heuristic[task][current_instance]\n",
    "    print(\"    Total makespan for heuristic and instance \" + str(current_instance) + \": \" + str(current_makespan_heuristic))\n",
    "    current_makespan_prediction = 0.0\n",
    "    for task in range(0, task_amount):\n",
    "        current_makespan_prediction += makespan_for_instances_prediction[task][current_instance]\n",
    "    print(\"    Total makespan for prediction and instance \" + str(current_instance) + \": \" + str(current_makespan_prediction))\n",
    "    makespan_difference = current_makespan_heuristic - current_makespan_prediction\n",
    "    makespan_differences.append(makespan_difference)\n",
    "avg = np.mean(makespan_differences)\n",
    "print(\"The average difference between makespan values between heuristic and prediction is \" + str(avg))\n",
    "if avg > 0:\n",
    "    print(\"Heuristic takes longer.\")\n",
    "elif avg < 0:\n",
    "    print(\"Prediction takes longer.\")\n",
    "else:\n",
    "    print(\"Same performance.\")\n",
    "    \n",
    "    # makespan_for_instances_prediction[i][j] #i es la task, j es una instancia en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(makespan_per_machine_heuristic[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Multithreading version\n",
    "# import os\n",
    "\n",
    "# def train_and_persist_classifier(classifier_index):\n",
    "#     # Data is loaded.\n",
    "#     TRAINING_FILE = baseDir + 'training/' + str(classifier_index) + '.csv'\n",
    "#     TEST_FILE = baseDir + 'test/' + str(classifier_index) + '.csv'\n",
    "#     training_set = pd.read_csv(TEST_FILE, header=None, delimiter=',')\n",
    "#     test_set = pd.read_csv(TRAINING_FILE, header=None, delimiter=',')\n",
    "#     # Create dataframe for data and separate target.\n",
    "#     df_training = pd.DataFrame(training_set)\n",
    "#     y_training = df_training.iloc[:, -1]\n",
    "#     # Validation/testing data is loaded.\n",
    "#     df_test = pd.DataFrame(test_set)\n",
    "#     y_test = df_test.iloc[:, -1]\n",
    "#     # Classifier is trained using the data.\n",
    "#     classifiers[classifier_index].fit(df_training.iloc[:, :-1], y_training)\n",
    "#     # Classifier directory is generated if it doesn't exist.\n",
    "#     generar_jobs.generate_dir(model_base_path)\n",
    "#     # Classifier is persisted.\n",
    "#     joblib.dump(classifiers[classifier_index], model_base_path + model_file_prefix + str(classifier_index) \\\n",
    "#                 + model_file_extension)\n",
    "#     # Classifier accuracy is determined using test data.\n",
    "#     results = []\n",
    "#     for i in range(0, len(df_test)):\n",
    "#         # Every test example is classified, and its classification is appended\n",
    "#         # to a results array.\n",
    "#         results.append(classifiers[classifier_index].predict(\n",
    "#             df_test.iloc[i][:-1].values.reshape(1, -1)))\n",
    "#     # Actual results are compared to expected values.\n",
    "#     accuracy = accuracy_score(y_test, results)\n",
    "#     os.write(1,'Classifier ' + str(classifier_index) + ':\\n') # Print directly to console\n",
    "#     os.write(1, 'Accuracy: ' + str(accuracy) + ', ') # Print directly to console\n",
    "#     # Calculated accuracy is added to accuracies list.\n",
    "#     accuracy_scores.append(accuracy)\n",
    "# #     os.write(1, 'Training of classifier ' + str(classifier_index) + ' finished.\\n') \n",
    "#     return\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     ##### VERSION 1 #####\n",
    "# #     jobs = []\n",
    "# #     for i in range(0, task_amount):\n",
    "# #         print('Starting training of classifier ' + str(i))\n",
    "# #         p = multiprocessing.Process(target=train_and_persist_classifier(i))\n",
    "# #         jobs.append(p)\n",
    "# #         p.start()\n",
    "#     ##### END VERSION 1 #####\n",
    "#     ##### VERSION 2 #####\n",
    "#     start = time.time()\n",
    "#     num_cores = multiprocessing.cpu_count() * 4\n",
    "#     # For every task, train a classifier.\n",
    "#     Parallel(n_jobs=num_cores)(delayed(train_and_persist_classifier)(i) for i in range(0,task_amount))\n",
    "#     end = time.time()\n",
    "#     print('The execution took ' + str(end - start) + ' seconds')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average accuracy (for all classifiers) is calculated (nothing to do with threading).\n",
    "promedio = 0.\n",
    "score_amount = len(accuracy_scores)\n",
    "for i in range(0, score_amount):\n",
    "    promedio += accuracy_scores[i]\n",
    "promedio /= score_amount\n",
    "print ('The average accuracy is {}'.format(promedio))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
